# Структура репозитория

```
1. data/
   ├── raw/                         # Исходные данные, генерируемые .sql-скриптами
   ├── interim/                     # Промежуточные таблицы после обработки
   ├── processed/                   # Итоговые таблицы, после определения значимых сегментов
   └── imgs/                        # Папка для сохранения изображений и графиков

2. scripts/                         # Папка с SQL-скриптами 
   ├── mr_nps_survey_results/
   └── kh_proc_cr_in_quiry_results/

3. scr/ 
   ├── utils/                       # Утилиты для вспомогательных функций
   │    ├── logging.py
   │    └── config_parser.py
   ├── pipeline/                    # Скрипты для обработки данных и анализа
   │    ├── dataset_generator.py
   │    ├── eda.py
   │    ├── preprocessing.py
   │    └── anomalies_detector.py
   
4. main.py                          # Основной файл для запуска всех этапов обработки данных
5. config.yaml                      # Конфигурационный файл для определения параметров (используются в `main.py`)
6. workflow.ipynb                   # Jupyter notebook, демонстрирующий работу из `main.py`
```

## Описание структуры репозитория

### 1. **`data/`**
   Эта папка хранит все данные на различных этапах обработки:
   - **`raw/`** — Исходные данные, которые включают таблицы, генерируемые SQL-скриптами. Если датасет генерируется, то необходимо два файла: `segments` с названиями колонок сегментов, и `segments_description.json`, включающий описание полей колонок-сегментов. Примеры этих файлов представлены в репозитории.
   - **`interim/`** — Промежуточные таблицы после обработки данных (включает заполнение пропусков, фильтрацию и т.п.)
   - **`processed/`** — Итоговые данные, которые содержат информацию о значимых сегментах и метриках. Результат работы для одной таблицы - три файла:
      - `{prefix}_significant_segments.csv` - основной файл с указанием значимых сегментов. Пример можно найти в блокноте `workflow.ipynb`
      - `{prefix}_significant_segments_z_scores.csv` - таблица с z-score для отклонений в данных.
      - `{prefix}_significant_segments_contribution.csv` - вклад изменения метрики группы внутри сегмента в изменение общей метрику.
   - **`imgs/`** — Здесь хранятся графики и изображения, генерируемые в процессе работы.

### 2. **`scripts/`**
   Папка, содержащая SQL-скрипты для обработки данных.
   - **`mr_nps_survey_results/`** — Скрипты для анализа результатов опросов NPS.
   - **`kh_proc_cr_in_quiry_results/`** — Скрипты для обработки данных по количеству обращений.


#### 2.1 Последовательность выполнения скриптов по опросам NPS:


   |  #  | Скрипт | Время обновления за месяц | Результат | 
| --- | ------ | ------------------------- | --------- | 
|  1  | Таблицы под дашборд NPS v2 (**НОВЫЕ CSI**) | до 50 минут | `mr_nps_segments_new_csi` | 
|  2  | Таблицы под дашборд NPS v2 | 10 минут | `mr_nps_survey_results_input` | 
|  3  | Таблицы под дашборд NPS v2 (**ТАРИФЫ**) | 20 минут | `mr_nps_survey_results_tariff` | 
|  4  | Таблицы под дашборд NPS v2 (**ПОМЕХИ**) | 2 минуты | `mr_nps_survey_radioblock` | 
|  5  | Таблицы под дашборд NPS v2 (**СЕЗОНКА**) | 50 минут | `fin_ba.mr_nps_segments_season_traff`, `mr_nps_segments_season_traf_seg` | 
|  6  | Таблицы под дашборд NPS **ВРЕМЕННО SMS для B2X** | 3 минуты | `mr_nps_survey_results_b2x_sms` | 
|  7  | Таблицы под дашборд NPS v2 (**УСЛУГИ**) | 2 минуты | `mr_nps_survey_results_service`, `mr_nps_survey_results_service_view` | 
|  8  | Таблицы под дашборд NPS v2 (**VAS**) | 1 минута | `mr_nps_survey_results_vas` | 
|  9  | Таблицы под дашборд NPS v2 (**OTT**) | 2 минуты | `mr_nps_survey_results_ott_view` | 
|  10  | Таблицы под дашборд NPS v2 (**ИТОГ**) | 20 минут | `mr_nps_survey_results` | 
|  **Итог**  | - | до 160 минут | `mr_nps_survey_results` |
   
#### 2.2 Последовательность выполнения скриптов по количеству обращений
|  #  | Скрипт | Время обновления за месяц | 
| --- | ------ | ---------------- | 
|  1  | `KH_CR_INQUIRY_DAILY` | до 20 минут | 
|  2  | **Обращаемость (ТАРИФЫ)** | 10 минут |
|  3  | **ОТТ** | 20 минут | 
|  4  | **Помехи** | 5 минут | 
|  5  | **VAS** | 50 минут |
|  6  | Итог | 180 минут |

### 3. **`src/`**
   Здесь находятся Python-скрипты для обработки и анализа данных. 
   - **`utils/`** — Вспомогательные функции для логирования и парсинга конфигураций.
   - **`pipeline/`** — Основные этапы обработки данных, включая генерацию датасетов, EDA, подготовку данных и поиск значимых сегментов. 
      - `dataset_generator.py` - генерация датасета, если данные не выгружаются из БД
      - `eda.py` - генерация графиков для анализа данных. Сохраняет изображения в папку `data/imgs`.
      - `preprocessing.py` - подготовка данных: заполнение пропусков, фильтрация и вычисление метрик.
      - `anomalies_detector.py`  - выявление значимых сегментов с использованием Isolation Forest и z-score-метода. 

### 4. **`main.py`**
   Основной скрипт, который запускает все этапы из `src/pipeline/`. Можете закомментировать ненужные части (например этап генерации датасета, если Вы выгружаете его из БД, у вас есть готовая таблица)

### 5. **`config.yaml`**
   Конфигурационный файл для задания общих параметров для всех этапов обработки данных.

### 6. **`workflow.ipynb`**
   Jupyter notebook, который демонстрирует работу скрипта `main.py` с примерами визуализации результатов и нескольких запусков с разными параметрами.
---

# Установка и зависимости

Для настройки и запуска проекта выполните следующие шаги:

### 1. Клонируйте репозиторий:

```bash
git clone https://github.com/Khinikadze/telecom-customer-satisfaction.git
cd telecom-customer-satisfaction
```

## 2. Создайте и активируйте виртуальное окружение:

Для создания виртуального окружения используйте Python 3.12 (рекомендуемая версия):

```bash
python3.12 -m venv venv
```

Для активации виртуального окружения выполните одну из следующих команд:

- На **Linux/macOS**:

```bash
source venv/bin/activate
```

- На **Windows**:

```bash
.\venv\Scripts\activate
```

### 3. Установите все необходимые зависимости:

После активации виртуального окружения установите все зависимости, указанные в `requirements.txt`:

```bash
pip install -r requirements.txt
```

Если вы хотите установить зависимости вручную, убедитесь, что у вас установлены библиотеки, перечисленные в `requirements.txt`. Чтобы убедиться, что все зависимости установлены корректно, можно выполнить:

```bash
pip list
```

Это выведет список установленных пакетов и их версии.

---

## Запуск скрипта

Для запуска основного процесса обработки данных выполните одну из следующих опций:

### Вариант 1: Запуск через CLI

1. Настройте конфигурационный файл `config.yaml`, указав необходимые параметры.
2. Для запуска основного пайплайна в командной строке выполните команду:

```bash
python main.py --config_path config.yaml
```

Это запустит все этапы обработки данных, указанные в файле `pipeline`.

### Вариант 2: Запуск через Jupyter Notebook

1. Откройте файл `workflow.ipynb` в Jupyter.
2. Выполните все ячейки. Параметры можно изменять как в `config.yaml`, так и вручную в самом блокноте.
